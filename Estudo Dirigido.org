#+OPTIONS: toc:nil num:nil

* Pesquisa para Estudo Dirigido II
** Proposta
    - Aplicar o conceito de aproximação local do LIME, mas ao invés de usar um modelo linear usar a entropia condicional (Informação mútua: I(Y|X)) das respostas do modelo.
*** DONE Qual a diferença? Vantagens e desvantagens?
    CLOSED: [2018-08-14 Ter 09:20]
    - [[https://stats.stackexchange.com/questions/360022/how-does-lime-compares-with-mutual-information][Pergunta no CV]]
    - Nenhuma resposta :c
*** O LIME usa modelos lineares e define \omega(g) como o número de coeficientes não-nulos
    - Seria possível usar aproximações de grau maior? Como interpretar?
*** Usar a Informação Mútua com as distribuições "discretizadas"? Ou usar aproximações para as distribuições contínuas?
** Plano de trabalho
*** Escolher um (ou mais) datasets
*** Treinar um (ou mais) modelos
*** Usar o LIME para explicar algumas instâncias
*** Escrever código para computar a informação mútua de perturbações na instâncias
*** Comparar explicações com as do LIME

** Artigo sobre XML Axiomático
*** [[https://arxiv.org/pdf/1808.05054.pdf][Link do Artigo]]
*** Define 3 axiomas para um explicação de ML
**** Identidade: Uma instância tem sempre a mesma explicação
**** Separabilidade: Instâncias não-idênticas tem explicações diferentes
    - =This axiom only holds if the model architecture does not have more degrees of freedom (DOF) than needed to represent the prediction function (Sundararajan et al., 2017).=
**** Estabilidade: Instâncias parecidas tem explicações parecidas
*** Quão razoáveis são esse axiomas?
    - Métodos de aproximação Local da Fronteira não podem garantir nenhum dos 3 (?)
    - Mudar o método de sampling para mão envolver amostragem aleatória poderia satisfazer o axioma 1
    - É razóavel acreditar que mesmo uma amostragem aleatória satisfaz os axiomas 2 e 3, desde que o número de instâncias amostradas seja o mesmo?

* Estratégia de Trabalho
** Fluxo principal
*** Treinar um modelo de ML sobre seus dados
*** Criar um explicador, passando uma interface de previsão (ex: .fit()) e o dataset
**** Explicador faz pré-precessamento
     - É necessário algum? Qual? Pode ser que mude? O código precisa ser flexível o suficiente para permitir isso?
     - Provavelmente pré-computar buckets para as distribuições?
*** Chamar a função de explicação para uma determinada instância
**** Gera perturbações no datapoint
     - Feito no Lime em =lime_tabular.py@381= (=__data_inverse=), tratamentos diferentes para features
       numéricas ("desnormaliza") e categóricas (1 se for igual a instância e 0 cc)[?]
**** Calcula informação mútua sobre a distribuição local
     - Com as perturbações geradas, fazer o bucketing de cada feature (categorizando-as), isso vai nos dar 
       uma distribuição discreta.
     - Como escolher o número ótimo de bins? [[https://en.wikipedia.org/wiki/Histogram#Number_of_bins_and_width][Wikipedia]]
     - A fórmula de Sturge k = log_2 n + 1 assume normalidade, é razoável?
     - numpy.linspace(min, max, k) e numpy.digitize(data, bins) [[https://docs.scipy.org/doc/numpy/reference/generated/numpy.digitize.html][Doc Digitize]]
     - Sample size ótimo? LIME faz um samplig uniforme (porquê?) estou usando 10%. Quem é o responsável (Explainer ou Preprocessor)
**** Gera um representação interpretável para isso
     
